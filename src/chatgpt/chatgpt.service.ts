import OpenAI from 'openai';
import { InternalServerErrorException, Logger } from '@nestjs/common';
import { Injectable } from '@nestjs/common';

@Injectable()
export class ChatgptService {
  private openai: OpenAI;
  private readonly logger = new Logger(ChatgptService.name);

  constructor() {
    const apiKey = process.env.OPENAI_API_KEY;
    
    if (!apiKey) {
      this.logger.error('La API key de OpenAI no est√° configurada');
      throw new Error('OPENAI_API_KEY no est√° configurada en las variables de entorno');
    }
    
    this.openai = new OpenAI({
      apiKey,
    });
    
    this.logger.log('Servicio de ChatGPT inicializado: o3 para c√≥digo, 4o para interpretaci√≥n');
  }

  /**
   * Genera respuestas usando o3 para generaci√≥n de c√≥digo Flutter/Angular
   * @param messages Array de mensajes con role and content
   * @param model Modelo a usar (por defecto o3 para mejor calidad)
   * @param temperature Temperatura para creatividad (por defecto 0.7)
   * @returns Respuesta del modelo
   */
  async chat(messages: Array<{ role: string; content: string }>, model = 'o3', temperature = 1): Promise<string> {
    try {
      this.logger.debug(`ü§ñ Generando c√≥digo con ${model} - ${messages.length} mensajes`);
      
      // Validar mensajes para OpenAI
      const validatedMessages = messages.map(msg => ({
        role: msg.role as 'system' | 'user' | 'assistant',
        content: msg.content
      }));
      
      // Configurar par√°metros seg√∫n el modelo
      const requestParams: any = {
        model,
        messages: validatedMessages,
      };

      // Configurar par√°metros seg√∫n el modelo
      if (model.startsWith('o3')) {
        // o3 necesita m√°s tokens: reasoning_tokens + completion_tokens
        requestParams.max_completion_tokens = 8000;
        requestParams.temperature = 1; // o3 solo acepta temperature = 1
      } else if (model === 'gpt-4o' || model === 'gpt-4o-mini') {
        // GPT-4o usa max_tokens est√°ndar
        requestParams.max_tokens = 4000;
        requestParams.temperature = temperature;
      } else {
        // Otros modelos (gpt-4, gpt-3.5-turbo, etc.)
        requestParams.max_tokens = 4000;
        requestParams.temperature = temperature;
      }

      const response = await this.openai.chat.completions.create(requestParams);

      const content = response.choices[0]?.message?.content || '';
      
      this.logger.debug(`‚úÖ Respuesta de ${model} recibida: ${content.length} chars`);
      
      // Debugging espec√≠fico para o3
      if (model.startsWith('o3')) {
        this.logger.debug(`üîç o3 response details: choices=${response.choices?.length}, finish_reason=${response.choices[0]?.finish_reason}`);
        if (content.length === 0) {
          this.logger.debug(`‚ùå o3 returned empty content. Full response:`, JSON.stringify(response, null, 2));
        }
      }
      
      return content;
    } catch (error) {
      this.logger.error(`‚ùå Error al llamar a la API de OpenAI: ${error.message}`, error.stack);
      
      if (error.status === 429) {
        throw new InternalServerErrorException('L√≠mite de solicitudes a OpenAI excedido. Intente de nuevo m√°s tarde.');
      }
      
      if (error.status === 400) {
        throw new InternalServerErrorException('Error en el formato de la solicitud a OpenAI. Verifique los par√°metros.');
      }
      
      if (error.status === 401) {
        throw new InternalServerErrorException('API key de OpenAI inv√°lida o expirada.');
      }
      
      throw new InternalServerErrorException(`Error al generar respuesta con ${model}: ${error.message}`);
    }
  }

  /**
   * M√©todo especializado para generaci√≥n de c√≥digo Flutter
   * Optimizado para prompts largos y respuestas complejas
   */
  async generateFlutterCode(systemPrompt: string, userPrompt: string): Promise<string> {
    // Intentar o3 primero con prompts completos SIN optimizar
    try {
      this.logger.debug('üöÄ Intentando generaci√≥n con o3...');
      
      // Enviar prompts COMPLETOS sin optimizaci√≥n
      this.logger.debug(`üìè Longitud prompts para o3: system=${systemPrompt.length}, user=${userPrompt.length}`);
      
      const messages = [
        { role: 'system', content: systemPrompt },
        { role: 'user', content: userPrompt }
      ];
      
      const result = await this.chat(messages, 'o3', 1);
      
      // Verificar que o3 devolvi√≥ contenido v√°lido
      if (result && result.trim().length > 100) {
        this.logger.debug(`‚úÖ o3 gener√≥ c√≥digo exitosamente (${result.length} chars)`);
        return result;
      } else {
        this.logger.warn(`‚ö†Ô∏è o3 devolvi√≥ respuesta vac√≠a o muy corta (${result?.length || 0} chars)`);
        throw new Error('o3 response too short or empty');
      }
    } catch (error) {
      this.logger.error(`‚ùå Error con o3: ${error.message}`);
      this.logger.debug('üîÑ Fallback a GPT-4o para generaci√≥n de c√≥digo...');
      
      // Fallback a GPT-4o con prompts originales
      const messages = [
        { role: 'system', content: systemPrompt },
        { role: 'user', content: userPrompt }
      ];
      
      return this.chat(messages, 'gpt-4o', 0.2);
    }
  }

  /**
   * M√©todo especializado para generaci√≥n de c√≥digo Angular
   * Optimizado para componentes y servicios Angular
   */
  async generateAngularCode(systemPrompt: string, userPrompt: string): Promise<string> {
    const messages = [
      { role: 'system', content: systemPrompt },
      { role: 'user', content: userPrompt }
    ];
    
    // o3 usa temperature = 1 autom√°ticamente
    return this.chat(messages, 'o3', 1);
  }

  /**
   * Genera respuestas usando GPT-4 Vision para an√°lisis de im√°genes
   * @param messages Array de mensajes que pueden incluir im√°genes
   * @param options Opciones adicionales como maxTokens y temperature
   * @returns Respuesta del modelo
   */
  async generateResponseWithVision(
    messages: Array<{ 
      role: 'system' | 'user' | 'assistant'; 
      content: string | Array<{ type: 'text' | 'image_url'; text?: string; image_url?: { url: string; detail?: string } }> 
    }>,
    options: { maxTokens?: number; temperature?: number } = {}
  ): Promise<string> {
    try {
      this.logger.debug(`üîç Analizando imagen con GPT-4 Vision - ${messages.length} mensajes`);
      
      const { maxTokens = 2000, temperature = 0.7 } = options;
      
      const response = await this.openai.chat.completions.create({
        model: 'gpt-4o', // gpt-4o tiene capacidades de visi√≥n
        messages: messages as any,
        max_tokens: maxTokens,
        temperature: temperature,
      });

      this.logger.debug(`‚úÖ Respuesta de GPT-4 Vision recibida correctamente`);
      
      return response.choices[0].message.content || '';
    } catch (error) {
      this.logger.error(`‚ùå Error al llamar a GPT-4 Vision: ${error.message}`, error.stack);
      
      if (error.status === 429) {
        throw new InternalServerErrorException('L√≠mite de solicitudes a OpenAI excedido. Intente de nuevo m√°s tarde.');
      }
      
      if (error.status === 400) {
        throw new InternalServerErrorException('Error en el formato de la solicitud a OpenAI Vision. Verifique la imagen y par√°metros.');
      }
      
      if (error.status === 401) {
        throw new InternalServerErrorException('API key de OpenAI inv√°lida o expirada.');
      }
      
      throw new InternalServerErrorException(`Error al analizar imagen con GPT-4 Vision: ${error.message}`);
    }
  }
}