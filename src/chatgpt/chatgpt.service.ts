import OpenAI from 'openai';
import { InternalServerErrorException, Logger } from '@nestjs/common';
import { Injectable } from '@nestjs/common';

@Injectable()
export class ChatgptService {
  private openai: OpenAI;
  private readonly logger = new Logger(ChatgptService.name);

  constructor() {
    const apiKey = process.env.OPENAI_API_KEY;
    
    if (!apiKey) {
      this.logger.error('La API key de OpenAI no est√° configurada');
      throw new Error('OPENAI_API_KEY no est√° configurada en las variables de entorno');
    }
    
    this.openai = new OpenAI({
      apiKey,
    });
    
    this.logger.log('Servicio de ChatGPT inicializado con o3');
  }

  /**
   * Genera respuestas usando o3 para generaci√≥n de c√≥digo Flutter/Angular
   * @param messages Array de mensajes con role y content
   * @param model Modelo a usar (por defecto o3 para mejor calidad)
   * @param temperature Temperatura para creatividad (por defecto 0.7)
   * @returns Respuesta del modelo
   */
  async chat(messages: Array<{ role: string; content: string }>, model = 'o3', temperature = 1): Promise<string> {
    try {
      this.logger.debug(`ü§ñ Generando c√≥digo con ${model} - ${messages.length} mensajes`);
      
      // Validar mensajes para OpenAI
      const validatedMessages = messages.map(msg => ({
        role: msg.role as 'system' | 'user' | 'assistant',
        content: msg.content
      }));
      
      // Configurar par√°metros seg√∫n el modelo
      const requestParams: any = {
        model,
        messages: validatedMessages,
      };

      // o3 tiene restricciones espec√≠ficas
      if (model.startsWith('o3')) {
        requestParams.max_completion_tokens = 4000;
        // o3 solo acepta temperature = 1 (valor por defecto)
        requestParams.temperature = 1;
      } else {
        requestParams.max_tokens = 4000;
        requestParams.temperature = temperature;
      }

      const response = await this.openai.chat.completions.create(requestParams);

      this.logger.debug(`‚úÖ Respuesta de ${model} recibida correctamente`);
      
      return response.choices[0].message.content || '';
    } catch (error) {
      this.logger.error(`‚ùå Error al llamar a la API de OpenAI: ${error.message}`, error.stack);
      
      if (error.status === 429) {
        throw new InternalServerErrorException('L√≠mite de solicitudes a OpenAI excedido. Intente de nuevo m√°s tarde.');
      }
      
      if (error.status === 400) {
        throw new InternalServerErrorException('Error en el formato de la solicitud a OpenAI. Verifique los par√°metros.');
      }
      
      if (error.status === 401) {
        throw new InternalServerErrorException('API key de OpenAI inv√°lida o expirada.');
      }
      
      throw new InternalServerErrorException(`Error al generar respuesta con ${model}: ${error.message}`);
    }
  }

  /**
   * M√©todo especializado para generaci√≥n de c√≥digo Flutter
   * Optimizado para prompts largos y respuestas complejas
   */
  async generateFlutterCode(systemPrompt: string, userPrompt: string): Promise<string> {
    const messages = [
      { role: 'system', content: systemPrompt },
      { role: 'user', content: userPrompt }
    ];
    
    // o3 usa temperature = 1 autom√°ticamente
    return this.chat(messages, 'o3', 1);
  }

  /**
   * M√©todo especializado para generaci√≥n de c√≥digo Angular
   * Optimizado para componentes y servicios Angular
   */
  async generateAngularCode(systemPrompt: string, userPrompt: string): Promise<string> {
    const messages = [
      { role: 'system', content: systemPrompt },
      { role: 'user', content: userPrompt }
    ];
    
    // o3 usa temperature = 1 autom√°ticamente
    return this.chat(messages, 'o3', 1);
  }
}